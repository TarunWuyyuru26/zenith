{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e871d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required modules\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Loading the environment variables from a .env file\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# LANGSMITH TRACKING\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24eac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x1131d1160> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1131d1be0> root_client=<openai.OpenAI object at 0x11287e7b0> root_async_client=<openai.AsyncOpenAI object at 0x1131d1940> model_name='o3-mini' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "# Select a chat model\n",
    "llm = ChatOpenAI(model_name=\"o3-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ea16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarunwuyyuru/Battinson/GenAI_Work/projects/zenith/.venv/lib/python3.13/site-packages/pydantic/v1/main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI refers to artificial intelligence systems that operate like agents—they perceive their environment, make decisions, and take autonomous actions to achieve specific goals without needing constant human oversight. In other words, rather than executing merely preprogrammed responses, agentic AI has the capacity to assess situations, plan strategies, and adapt to changing circumstances in order to move toward a particular objective.\n",
      "\n",
      "Here are some key aspects of agentic AI:\n",
      "\n",
      "1. Autonomy: These systems aren't tightly bound by a fixed set of instructions. Instead, they have the ability to determine actions on their own based on their interactions with the world.\n",
      "\n",
      "2. Goal-Directed Behavior: Agentic AI is designed to pursue certain objectives. Whether it's navigating a robot through a varying terrain, playing a complex game, or managing resources in a simulated environment, the AI is oriented toward achieving specific outcomes.\n",
      "\n",
      "3. Sensing and Perception: Like living agents, these AI systems typically rely on sensors (or data inputs) to gather information about their surroundings. This perception is then translated into actionable information that guides their behavior.\n",
      "\n",
      "4. Adaptability and Learning: Many agentic AI systems incorporate learning mechanisms, such as reinforcement learning or other adaptive methods, which allow them to improve their performance over time. They adjust their behavior based on past experiences and new data.\n",
      "\n",
      "Research in AI often contrasts agentic AI with more reactive or rule-based systems, emphasizing the former’s ability to act in complex, dynamic environments. While agentic AI offers impressive potential—enabling systems to handle unpredictable or nuanced scenarios—it also raises important discussions about control, accountability, and safety, especially as these agents become more autonomous.\n",
      "\n",
      "In summary, agentic AI is about creating systems that can think and act as independent agents, capable of intelligent behavior that adapts according to the demands of their environment and the goals they've been set.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the LLM with a simple query\n",
    "result = llm.invoke(\"What is agentic AI?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9edde71",
   "metadata": {},
   "source": [
    "---\n",
    "# Basic Chat Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8759dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a797cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a suite of tools developed by LangChain aimed at improving the development, testing, and evaluation of applications that utilize language models. It provides a variety of features to support developers in building and maintaining robust, reliable LLM applications. Some key aspects of Langsmith include:\n",
      "\n",
      "1. **Tracking and Observability:** Langsmith offers tools to monitor and visualize the behavior of applications that use language models. This includes tracking inputs, outputs, and other metrics that are important for understanding how the application is performing in real-time.\n",
      "\n",
      "2. **Debugging:** With debugging tools integrated into Langsmith, developers can more easily diagnose and fix issues that arise in their language model applications. This can involve analyzing logs, understanding failures, and reproducing errors for more effective troubleshooting.\n",
      "\n",
      "3. **Testing:** Langsmith provides a framework for systematically testing the functionality of LLM applications. This ensures that new features or changes do not introduce bugs or regressions by enabling developers to run automated tests.\n",
      "\n",
      "4. **Evaluation:** Through evaluation tools, developers can measure the effectiveness and accuracy of their language models. Langsmith supports both quantitative and qualitative evaluations, helping developers refine their models based on performance data.\n",
      "\n",
      "Overall, Langsmith is designed to enhance the efficiency and quality of applications that rely on language models, making it easier for developers to manage the complexities involved in their development and deployment. Its features are particularly valuable for maintaining the integrity and performance of sophisticated language-based systems.\n"
     ]
    }
   ],
   "source": [
    "# Selecting a more powerful model\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Chaining the prompt with the LLM\n",
    "chain=prompt|llm \n",
    "\n",
    "# Invoking the chain with an input\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "\n",
    "# print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ece7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a tool designed to enhance the development experience when working with language models. It aids developers in building, evaluating, and monitoring applications powered by these models. Langsmith provides features that allow for seamless integration, debugging, and performance analysis of language model-based applications.\n",
      "\n",
      "Key components include environments for iterative testing, utilities for monitoring model outputs, and mechanisms for measuring and improving model performance. Langsmith's integration capabilities are particularly crucial for aligning language model responses with specific application requirements. It often forms a part of the broader ecosystem supporting natural language processing and machine learning development efforts.\n",
      "\n",
      "Langsmith is typically used in conjunction with frameworks such as LangChain, which is designed for building applications that use language models as a central component. With these tools, developers can construct complex systems that leverage AI capabilities for tasks like natural language understanding, text generation, and more.\n",
      "\n",
      "Overall, Langsmith provides infrastructure and tools that streamline the process of creating robust and scalable applications based on language models.\n"
     ]
    }
   ],
   "source": [
    "# Let's take a step further and add an output parser to get the response as a simple string.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "# Chaining the prompt, LLM and output parser\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "# Invoking the chain with an input\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93c5356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is a state-of-the-art development framework and platform designed for building and managing LLM (Language Model) applications. It provides developers with a comprehensive set of tools and features to create, fine-tune, and deploy language model applications effectively.', 'features': [{'name': 'Development Framework', 'description': 'Offers a flexible, powerful set of tools for constructing LLM applications, streamlining the development process.'}, {'name': 'Integration Capabilities', 'description': 'Seamlessly integrates with various ML frameworks and APIs, providing extensive support for custom applications.'}, {'name': 'Scalability', 'description': 'Designed to handle applications of varying scales, ensuring efficient resource management and performance optimization.'}], 'benefits': ['Accelerates development of LLM applications with advanced tools.', 'Improves application performance with robust management features.', 'Enhances flexibility and customizability for developers.'], 'target_users': ['Data scientists', 'Machine learning engineers', 'Developers interested in NLP applications']}\n"
     ]
    }
   ],
   "source": [
    "# Let's create a prompt template with a JSON output parser.\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Chaining the prompt, LLM and output parser\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2247158",
   "metadata": {},
   "source": [
    "---\n",
    "# Retrival Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff31121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Loading documents from a web page\n",
    "loader=WebBaseLoader(\"https://python.langchain.com/docs/tutorials/llm_chain/\")\n",
    "\n",
    "# Loading the content of the web page into as documents\n",
    "documents=loader.load()\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1858750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(documents=documents)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05453b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f48cbf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x114ba9be0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(documents,embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e03afb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
      "This tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n"
     ]
    }
   ],
   "source": [
    "# Now let's perform a similarity search on the vector store we just created.\n",
    "\n",
    "query=\"This is a relatively simple LLM application\"\n",
    "\n",
    "result=vectorstore.similarity_search(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's give a proper context to the LLM for the similarity search.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade52a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11365df30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11365e2c0>, root_client=<openai.OpenAI object at 0x11365e060>, root_async_client=<openai.AsyncOpenAI object at 0x11365de00>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c369295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
      "This tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n"
     ]
    }
   ],
   "source": [
    "# Now let's perform a similarity search on the vector store we just created.\n",
    "\n",
    "new_query=\"what is  LLM application\"\n",
    "new_result=vectorstore.similarity_search(new_query)\n",
    "print(new_result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccef6b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the following question and answer can be constructed:\\n\\n**Question:** What is the purpose of using LangSmith in building applications with LangChain?\\n\\n**Answer:** The purpose of using LangSmith in building applications with LangChain is to inspect and trace what exactly is happening inside the chain or agent, especially as the applications become more complex with multiple steps and multiple invocations of LLM calls. LangSmith allows developers to log traces, which helps in understanding the internal workings of the application.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke({\"context\":new_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b758a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18f57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d17a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
